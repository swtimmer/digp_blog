---
title: "Evaluating Machine Learning models when dealing with imbalanced classes "
author: "Sander Timmer, PhD - Data Scientist @ Microsoft"
date: "February 26, 2016"
output: html_document
---

This is the R code related to the blog post written on the Data Insights Global Practice MSDN blog. You can find the fill post with more background here:

# loading data and setting enviroment

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(reshape2)
library(data.table)
theme_set(theme_bw())

#Read the data
dataset <- read.csv("ScoredData.csv", header = TRUE)
head(dataset)

```



## Define and calculate avaluation metrics 

First we created some code that can calculate the Cohen's Kappa coefficient (kappa) and some other basic evaluation metrics. 

```{r kappa}
#some simple code to calculate the Kappa
kappa = function(TP, TN, FP, FN)
{
  N = TP+TN+FP+FN
  #Probability observed
  Po = (TP + TN) / N
  #Probability expected
  Pe = (((TN+FP) * (TN+FN)) + ((FN+TP) * (FP + TP))) / (N * N)
  #Kappa
  kappa = (Po - Pe) / (1-Pe)
  kappa
}

scoreTreshold = function(threshold, dataset){
  n = nrow(dataset)
  
  #make the confusion matrix
  index = which(dataset$Scored.Probabilities < threshold)
  FN = as.numeric(length(which(dataset$label[index] == 1)))
  TN = as.numeric(length(which(dataset$label[index] == 0)))
  TP = as.numeric(length(which(dataset$label[-index] == 1)))
  FP = as.numeric(length(which(dataset$label[-index] == 0)))

  #some generic evaluation metrics
  Precision = TP / (TP + FP)
  Recall =  TP / (TP + FN)
  F1 = (Recall * Precision)/(Recall + Precision)
  k = kappa(TP, TN, FP, FN)
  accuracy = (TP + TN)/n
  
  #create a data.frame to export from this function
  out = data.frame(
    Threshold=threshold,N=n, Accuracy = accuracy, TP = TP, TN = TN, FP =  FP, FN = FN,
    Precision = Precision,
    Recall =  Recall,
    F1 = F1,
    kappa = k
  )
  out

}

```


### Running the metrics


```{r runkappa}
# Run the Kappa test for various Thresholds
toTest= seq(from=0, to = 1, by = 0.01)
toTest.res = lapply(toTest, FUN=scoreTreshold, dataset)
toTest.res = rbindlist(toTest.res)

```

### plot the outcome of the metrics

We now generate a few plots that show how the different evaluation metrics change by picking a different threshold. 

```{r plotthemetric}
#plot Threshold vs F1 and Accuracy 
qplot(data = toTest.res, x=Threshold, y=Accuracy,  geom="point")
qplot(data = toTest.res, x=Threshold, y=F1, geom="point")

#plot Threshold vs Kappa
qplot(data = toTest.res, x=Threshold, y=kappa, geom="point")

#compare F1 to Kappa
qplot(data = toTest.res, x=F1, y=kappa, geom="point", colour=Threshold)

#more plots just because we can! 
qplot(data = toTest.res, x=Threshold, y=Precision, colour=kappa, geom="point")
qplot(data = toTest.res, x=Threshold, y=Recall, colour=kappa, geom="point")
  
```

### Pick the best Threshold giving the highest Kappa
Now we have our metrics for our model we can select which model we "prefer" or at least consider best given the different metrics. Below I show selecting the best model threshold for kappa metric. 

```{r selectkappabest}
#best F1
toTest.res[which.max(toTest.res$F1),]
#best kappa
toTest.res[which.max(toTest.res$kappa),]

```

## Weighted kappa

We now can do the same analyses with a weighted Kappa. Doing so, we can give more emphasis on what we find important. For example, FN is a very expensive case and maybe twice as expensive as a FP (callback versus unwanted maintenance). By adding weights to each of the outcomes of the confusion matrix we can now re-scale our prop table onto that. 

To illustrate this effect I decided to take the following weights:
TP = 1
TN = 1
FP = 0
FN = 2

```{r kappaw}

kappalw = function(TP, TN, FP, FN,TPw, TNw, FPw, FNw)
{
  N = TP * TPw +TN * TNw +FP *FPw +FN * FNw
  #Probability observed with weights
  Po = (TP * TPw + TN * TNw) / N
  #Probability expected with weights
  Pe = (((TN * TNw+FP * FPw) * (TN * TNw + FN * FNw)) + ((FN * FNw + TP * TPw) * (FP * FPw + TP * TPw))) / (N * N)
  #Weigthed Kappa
  kappalw = (Po - Pe) / (1-Pe)
  kappalw
  
}


scoreTreshold = function(threshold, dataset){
  n = nrow(dataset)
  
  
  index = which(dataset$Scored.Probabilities < threshold)
  
  FN = as.numeric(length(which(dataset$label[index] == 1)))
  TN = as.numeric(length(which(dataset$label[index] == 0)))
  
  TP = as.numeric(length(which(dataset$label[-index] == 1)))
  FP = as.numeric(length(which(dataset$label[-index] == 0)))
  
  Precision = TP / (TP + FP)
  Recall =  TP / (TP + FN)
  F1 = (Recall * Precision)/(Recall + Precision)
  k = kappa(TP, TN, FP, FN)
  klw = kappalw(TP, TN, FP, FN, 1, 1, 0, 2)
  
  accuracy = (TP + TN)/n
  
  out = data.frame(
    Threshold=threshold,N=n, Accuracy = accuracy, TP = TP, TN = TN, FP =  FP, FN = FN,
    
    Precision = Precision,
    Recall =  Recall,
    F1 = F1,
    kappa = k,
    kappalw = klw
  )
  out

}

#Run the same thresholds but now weighted
toTest.res = lapply(toTest, FUN=scoreTreshold, dataset)
toTest.res = rbindlist(toTest.res)


```

We now observe that some previously not soo great scoring thresholds are suddenly becoming more interesting as they will yield less FalseNegatives which we are extremely tough on right now. 

```{r plotthemetrickappalw}

qplot(data = toTest.res, x=kappalw, y=kappa, geom="point", colour=Threshold)
qplot(data = toTest.res, x=kappalw, y=F1, geom="point", colour=Threshold)


```
  

## Scenarios for Cost sensitive predictions

We are now going to compare the weighted Kappa tests for different cost scenarios. 

|   | TP  | TN  | FP  | FN   | Reason |
|---|---|---|---|---|---|
|  Scenario1 |  1 | 1  |  1 | 1  | Balanced model|
|  Scenario3 |  2 | 1  |  1 | 2  | Favour TP & TN |
| Scenario3|  2 | 0.1  |  0.1 | 2  | Favour TP & FN |
| Scenario4|  5 | 1  |  1 | 5  | Favour TP & FN extra |
| Scenario5|  2 | 0.1  |  1 | 3  | FN are most costly |
| Scenario6| 0.1 | 3  |  2 | 0.1  | Favour predicting TN & FP |



```{r modelopt}


scoreTreshold = function(threshold, dataset,TPw, TNw, FPw, FNw){
  n = nrow(dataset)
  
  
  index = which(dataset$Scored.Probabilities < threshold)
  
  FN = as.numeric(length(which(dataset$label[index] == 1)))
  TN = as.numeric(length(which(dataset$label[index] == 0)))
  
  TP = as.numeric(length(which(dataset$label[-index] == 1)))
  FP = as.numeric(length(which(dataset$label[-index] == 0)))
  
  
  Precision = TP / (TP + FP)
  Recall =  TP / (TP + FN)
  F1 = (Recall * Precision)/(Recall + Precision)
  k = kappa(TP, TN, FP, FN)
  klw = kappalw(TP, TN, FP, FN,TPw, TNw, FPw, FNw)
  
  accuracy = (TP + TN)/n
  
  out = data.frame(
    Threshold=threshold,N=n, Accuracy = accuracy, TP = TP, TN = TN, FP =  FP, FN = FN,
    
    Precision = Precision,
    Recall =  Recall,
    F1 = F1,
    kappa = k,
    kappalw = klw
  )
  out

}


#Let's run the different scenarios:
#Run the same thresholds but now weighted
toTest.res = lapply(toTest, FUN=scoreTreshold, dataset, 1 , 1, 1, 1)
toTest.res = rbindlist(toTest.res)
toTest.res$scenario = rep("Scenario1", nrow(toTest.res))
toTest.all = toTest.res

toTest.res = lapply(toTest, FUN=scoreTreshold, dataset, 2 , 1, 1, 2)
toTest.res = rbindlist(toTest.res)
toTest.res$scenario = rep("Scenario2", nrow(toTest.res))
toTest.all = rbind(toTest.all,toTest.res)

toTest.res = lapply(toTest, FUN=scoreTreshold, dataset, 2 , 0.1, 0.1, 2)
toTest.res = rbindlist(toTest.res)
toTest.res$scenario = rep("Scenario3", nrow(toTest.res))
toTest.all = rbind(toTest.all,toTest.res)

toTest.res = lapply(toTest, FUN=scoreTreshold, dataset, 5 ,1, 1, 5)
toTest.res = rbindlist(toTest.res)
toTest.res$scenario = rep("Scenario4", nrow(toTest.res))
toTest.all = rbind(toTest.all,toTest.res)

toTest.res = lapply(toTest, FUN=scoreTreshold, dataset, 2 ,0.1, 1, 4)
toTest.res = rbindlist(toTest.res)
toTest.res$scenario = rep("Scenario5", nrow(toTest.res))
toTest.all = rbind(toTest.all,toTest.res)

toTest.res = lapply(toTest, FUN=scoreTreshold, dataset, 0.1 ,3, 2, 0.1)
toTest.res = rbindlist(toTest.res)
toTest.res$scenario = rep("Scenario6", nrow(toTest.res))  
toTest.all = rbind(toTest.all,toTest.res)  

```
### Comparing the different cost scenarios
In the following plot is to see for the different weighing we will be able to determine a different optimal threshold in how we should use the model in production. 

```{r plotsentive}

# Transpose our wide data.frame into a long data.frame as needed for ggplot2
toTest.long = as.data.frame(toTest.all)[,c(1,12,13)]
toTest.long = melt(toTest.long,id=c("scenario","Threshold"),variable_name="kappalw")

#rename the 4th column to something that makes more sense
colnames(toTest.long)[4] = "KappaLW"

#now plot the various scenarios next to each other
qplot(data=toTest.long, x=Threshold, y=KappaLW, colour=scenario, geom="point")


```




